{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Kernel Methods in Machine Learning\n",
    "\n",
    "<img src='http://www.sbaban.org/wp-content/uploads/2013/11/svm.jpg' />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Some Definitions\n",
    "\n",
    "1. ###Inner product space\n",
    " - ###Inner product\n",
    "2. ###Hilbert space\n",
    " - ###Complete\n",
    " - ###Separable\n",
    "3. ###Reproducing kernel Hilbert space\n",
    " - ###Reproducing property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mercer's Theorem (Mercer 1909)\n",
    "Let $ \\mathcal{X} $ be a compact space equipped with a strictly positive finite Borel measure $ \\mu $ and $ K: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R} $ a continuous, symmetric, and positive definite function.  Define the integral operator $ T_K: L^2(\\mathcal{X}) \\rightarrow L^2(\\mathcal{X}) $ as \n",
    "\n",
    "$$ [T_K f](\\cdot) =\\int_\\mathcal{X}  K(\\cdot,t) f(t)\\, d\\mu(t) $$\n",
    "\n",
    "where $ L^2(\\mathcal{X}) $ is the space of square integrable functions with respect to $ \\mu $.\n",
    "\n",
    "Then $ K $ can be written in terms of the eigenvalues and continuous eigenfunctions of $T_k$ as \n",
    "\n",
    "$$ K(x,y) = \\sum_{j=1}^\\infty \\lambda_j \\, \\phi_j(x) \\, \\phi_j(y) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Moore-Aronszajn Theorem (Aronzajn 1950)\n",
    "\n",
    "Suppose $K$ is a symmetric, positive definite kernel on a set $\\mathcal{X}$. Then there is a unique Hilbert space of functions on $\\mathcal{X}$ for which $K$ is a reproducing kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Representer Theorem (Kimeldorf and Wahba 1971)\n",
    "\n",
    "Let $\\mathcal{X}$ be a nonempty set and $k$ a positive-definite real-valued kernel on $\\mathcal{X} \\times \\mathcal{X}$ with corresponding reproducing kernel Hilbert space $H_k$.  Given a training sample $(x_1, y_1), \\dotsc, (x_n, y_n) \\in \\mathcal{X} \\times \\mathbb{R}$, a strictly monotonically increasing real-valued function $g \\colon [0, \\infty) \\to \\mathbb{R}$, and an arbitrary empirical risk function $E \\colon (\\mathcal{X} \\times \\mathbb{R}^2)^n \\to \\mathbb{R} \\cup \\lbrace \\infty \\rbrace$, then for any $f^{*} \\in H_k$ satisfying\n",
    "\n",
    "$$\n",
    " f^{*} = \\operatorname{arg min}_{f \\in H_k} \\left\\lbrace E\\left( (x_1, y_1, f(x_1)), ..., (x_n, y_n, f(x_n)) \\right) + g\\left( \\lVert f \\rVert \\right) \\right \\rbrace,\n",
    "$$\n",
    "\n",
    "$f^{*}$ admits a representation of the form:\n",
    "\n",
    "$$\n",
    " f^{*}(\\cdot) = \\sum_{i = 1}^n \\alpha_i k(\\cdot, x_i),\n",
    "$$\n",
    "\n",
    "where $\\alpha_i \\in \\mathbb{R}$ for all $1 \\le i \\le n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Advantages of RKHS in Machine Learning\n",
    "\n",
    "- ##Powerful and flexible models can be defined.\n",
    "\n",
    "- ##Many results and algorithms for linear models in Euclidean spaces can be generalized to RKHS.\n",
    "\n",
    "- ##Learning theory assures that effective learning in RKHS is possible, for instance, by means of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Example - Support Vector Machine\n",
    "\n",
    "<iframe width=\"420\" height=\"315\" src=\"//www.youtube.com/embed/3liCbRZPrZA\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Example - Ridge Regression\n",
    "\n",
    "###Objective : Minimize $\\|A\\mathbf{x}-\\mathbf{b}\\|^2_2+ \\lambda\\| \\mathbf{x}\\|^2_2$\n",
    "\n",
    "###Explicit solution : $\\hat{x} = (A^{T}A+ \\lambda I )^{-1}A^{T}\\mathbf{b}$\n",
    "\n",
    "###Kernelization $\\implies$ $\\hat{x} = (K+ \\lambda I )^{-1}A^{T}\\mathbf{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Example - Principal Component Analysis\n",
    "\n",
    "Objective : Compute eigendecomposition of $X^T X = W \\Sigma W^T$, where $\\Sigma$ is diagonal matrix which keeps eigenvalues.\n",
    "\n",
    "Kernelization $\\implies$ Replacing $X^T X$ by $K$.\n",
    "<table border=\"0\">\n",
    "<tr>\n",
    "<td><img src='http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_pca_input.png/300px-Kernel_pca_input.png' /></td>\n",
    "<td><img src='http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Kernel_pca_output.png/300px-Kernel_pca_output.png' /></td>\n",
    "<td><img src='http://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Kernel_pca_output_gaussian.png/300px-Kernel_pca_output_gaussian.png' /></td>\n",
    "</tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "author": "Hsueh-Min Chen",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
